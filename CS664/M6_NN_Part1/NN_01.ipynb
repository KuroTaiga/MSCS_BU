{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Introduction to Neural Networks\"\n",
    "format: html\n",
    "page-layout: full\n",
    "code-line-numbers: true\n",
    "code-block-border: true\n",
    "toc: true\n",
    "toc-location: left\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Neural Network with one hidden layer\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_01_12.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QgyxAMQ25SsF"
   },
   "source": [
    "- a simple neural network that will classify the Iris flower dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LGwJJgnY5FJ-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', \n",
    "                      names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "dataset['species'] = pd.Categorical(dataset['species']).codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  species\n",
       "0             5.1          3.5           1.4          0.2        0\n",
       "1             4.9          3.0           1.4          0.2        0\n",
       "2             4.7          3.2           1.3          0.2        0\n",
       "3             4.6          3.1           1.5          0.2        0\n",
       "4             5.0          3.6           1.4          0.2        0\n",
       "..            ...          ...           ...          ...      ...\n",
       "145           6.7          3.0           5.2          2.3        2\n",
       "146           6.3          2.5           5.0          1.9        2\n",
       "147           6.5          3.0           5.2          2.0        2\n",
       "148           6.2          3.4           5.4          2.3        2\n",
       "149           5.9          3.0           5.1          1.8        2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LGwJJgnY5FJ-"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.9</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.2</td>\n",
       "      <td>2.7</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  species\n",
       "91            6.1          3.0           4.6          1.4        1\n",
       "63            6.1          2.9           4.7          1.4        1\n",
       "103           6.3          2.9           5.6          1.8        2\n",
       "6             4.6          3.4           1.4          0.3        0\n",
       "59            5.2          2.7           3.9          1.4        1\n",
       "..            ...          ...           ...          ...      ...\n",
       "143           6.8          3.2           5.9          2.3        2\n",
       "116           6.5          3.0           5.5          1.8        2\n",
       "53            5.5          2.3           4.0          1.3        1\n",
       "38            4.4          3.0           1.3          0.2        0\n",
       "47            4.6          3.2           1.4          0.2        0\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle the dataset\n",
    "\n",
    "dataset = dataset.sample(frac=1, random_state=1234)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LGwJJgnY5FJ-"
   },
   "outputs": [],
   "source": [
    "# split the data set into train and test subsets\n",
    "# Use 120 samples for training and 30 samples for testing\n",
    "\n",
    "train_input = dataset.values[:120, :4]\n",
    "train_target = dataset.values[:120, 4]\n",
    "\n",
    "test_input = dataset.values[120:, :4]\n",
    "test_target = dataset.values[120:, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pccpZgHU8iAB"
   },
   "source": [
    "- Define a feedforward network with one hidden layer with five units,\n",
    "- a ReLU activation function, *f(x) = max(0, x)*,\n",
    "- and an output layer with three units.\n",
    "    - The output layer has three units, whereas each unit corresponds to one of the three classes of Iris flower.\n",
    "    \n",
    "- The following is the PyTorch definition of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_dOD1OMg8knA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "hidden_units = 5\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, hidden_units), # we'll use a network with 5 hidden units\n",
    "    torch.nn.ReLU(), # ReLU activation\n",
    "    torch.nn.Linear(hidden_units, 3) # 3 output units for each of the 3 possible classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri9tmJd_87qB"
   },
   "source": [
    "- We'll use one-hot encoding for the target data.\n",
    "- each class of the flower will be represented as an array\n",
    "        - (Iris Setosa = [1, 0, 0], Iris Versicolour = [0, 1, 0], and Iris Virginica = [0, 0, 1]),\n",
    "- and one element of the array will be the target for one unit of the output layer.\n",
    "- When the network classifies a new sample, we'll determine the class by taking the unit with the highest activation value. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iMaXLyq-ES2"
   },
   "source": [
    "- Define the loss function\n",
    "  - The loss function will measure how different the output of the network is compared to the target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "SiqLpoz6-Ic9"
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFxHfeDl-ZLC"
   },
   "source": [
    "- Define the optimizer\n",
    "   - stochastic gradient descent (SGD) optimizer (a variation of the gradient descent algorithm) with a learning rate of 0.1 and a momentum of 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "iCGhvKK0-5DT"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.1, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqVJH4UU_SH-"
   },
   "source": [
    "We'll run the training for 50 epochs, which means that we'll iterate 50 times over the training dataset: \n",
    "\n",
    "\n",
    "1.   Create the torch variable that are `input` and `target` from the numpy array train_input and train_target. \n",
    "2.   Zero the gradients of the optimizer to prevent accumulation from the previous iterations. We feed the training data to the neural network net (input) and we compute the loss function criterion (out, targets) between the network output and the target data.\n",
    "3.   Propagate the loss value back through the network. We do this so that we can calculate how each network weight affects the loss function. \n",
    "4.   The optimizer updates the weights of the network in a way that will reduce the future loss function values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRkD7513_I9i",
    "outputId": "ca3ee637-fc47-4e44-ceea-6791b95ca248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.2181\n",
      "Epoch 10 Loss: 0.6745\n",
      "Epoch 20 Loss: 0.2447\n",
      "Epoch 30 Loss: 0.1397\n",
      "Epoch 40 Loss: 0.1001\n",
      "Epoch 50 Loss: 0.0855\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    inputs = torch.autograd.Variable(torch.Tensor(train_input).float())\n",
    "    targets = torch.autograd.Variable(torch.Tensor(train_target).long())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    out = net(inputs)\n",
    "    loss = criterion(out, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        print('Epoch %d Loss: %.4f' % (epoch + 1, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVfDIxE6AJpC"
   },
   "source": [
    "Let's see what the final accuracy of our model is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13bt21qAALGI",
    "outputId": "012148f3-d2ba-443f-fe17-a41d71c77661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errors: 0; Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = torch.autograd.Variable(torch.Tensor(test_input).float())\n",
    "targets = torch.autograd.Variable(torch.Tensor(test_target).long())\n",
    "\n",
    "optimizer.zero_grad()\n",
    "out = net(inputs)\n",
    "_, predicted = torch.max(out.data, 1)\n",
    "\n",
    "error_count = test_target.size - np.count_nonzero((targets == predicted).numpy())\n",
    "print('Errors: %d; Accuracy: %d%%' % (error_count, 100 * torch.sum(targets == predicted) / test_target.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math behind Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector\n",
    " - a one-dimensional array of numbers\n",
    "   \n",
    "   ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/34.png)\n",
    "\n",
    "- Magnitude or length of a vector\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/37.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix\n",
    "\n",
    "- a two dimensional array of scalars\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/39.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor\n",
    "\n",
    "- A multi-dimensional array with the following properties\n",
    "    - rank  (the number of array dimensions)\n",
    "    - shape (the size of each of the tensor's dimensions)\n",
    "    - data type  (the data type of the tensor values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector and matrix operations\n",
    "\n",
    " - Vector addition\n",
    "\n",
    "   ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/40.png)\n",
    "   \n",
    " - Dot product (or scalar product)\n",
    "     - combines two *n*-dimensional vectors **a** and **b** into a scalar value \n",
    "\n",
    "   ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/41.png)\n",
    "\n",
    "    - If the vectors are two-dimensional\n",
    "  \n",
    "     ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/45.png)\n",
    "\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_02.jpg)\n",
    "         \n",
    " - Cross product (or vector product)\n",
    "     - a new vector perpendicual to both the input vectors\n",
    "     - the magnitude of the output vector is equal to\n",
    "  \n",
    "       ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/48.png)\n",
    "\n",
    "        ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_03.jpg)\n",
    " \n",
    "      - the output vector magnitude to equal to the area of the parallelogram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Matrix transpose\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/55.png)\n",
    "  \n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/56.png)\n",
    "  \n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/57.png)\n",
    "  \n",
    "\n",
    "- Matrix-scalar multiplication\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/58.png)\n",
    "\n",
    "- Matrix-matrix addition\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/59.png)\n",
    "\n",
    "- Matrix-vector multiplication\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/60.png)\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/61.png)\n",
    "\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/62.png)\n",
    "\n",
    "\n",
    "- Matrix-matrix multiplication\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/63.png)\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/64.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Units of Neural Networks\n",
    "\n",
    " - The smallest building blocks\n",
    "\n",
    "   ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_08.jpg)\n",
    "   \n",
    "\n",
    "   ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/111.png)\n",
    "\n",
    " - Compute the weighted sum, $\\Sigma x_i w_i + b$, also known as the activation value\n",
    "     - the inputs $x_i$ represent either the outputs of other units of the network, or the values of the input data itself\n",
    "     - the weights $w_i$ represent either the strength of the inputs or the strengths of the connections between the units\n",
    "     - the weight $b$ is the **bias**, an always-on input unit with a value of 1\n",
    "  \n",
    " - The sum  $\\Sigma x_i w_i + b$ serves as input to the $f$, the **activation function** (**transfer function**)\n",
    "     - The output of $f$ is a single numerical value, representing the output of the unit itself\n",
    "  \n",
    " - A unit with an identify activation function, $f(x) = x$ is equivalent to **multiple linear regression**\n",
    " - A unit with a **sigmoid activation** function is equivalent to **logistic regression**\n",
    " - A unit with a threshold activation function is equivalent to a **perceptron (binary classifer)**\n",
    "     - $f(a) = 1 if a \\ge 0 else 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fully connected (FC) layer in classical Neural Networks\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_09.jpg)\n",
    "\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_10.jpg)\n",
    "\n",
    "  \n",
    "- Multi layer Neural Networks\n",
    "\n",
    " ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_11.jpg)\n",
    "\n",
    "\n",
    "   - A unit from one layer is connected to all the units from the previous and following layers. Each connection has its own weight\n",
    "   - The above consists of single-path networks with sequential layers\n",
    "   - The layers form **directed acyclic graphs**\n",
    "   - Randomly interconnected hidden layers\n",
    "\n",
    "     ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_12.jpg)\n",
    "\n",
    "   - $\\theta$, the set of all weight matrices\n",
    "   - A Neural network can be represented as a series of nested functions/operations\n",
    "\n",
    "     ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/138.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Without activation functions, the output would be the weighted sum of the inputs, a linear function\n",
    " - The entire network becomes a composition of linear functions, resulting in a linear function\n",
    " - Hence, the network would be equivalent to a simple linear regression model\n",
    " - For non-linear functions, use non-linear activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output bounded between 0 and 1\n",
    "- Can be interpreted as the probability of the unit being active\n",
    "  \n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_13.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperbolic tangent (tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Output is in the (-1, 1) range\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_14.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Unit (ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ReLU repeats its input when $x > 0$, and stays $0$ otherwise\n",
    "- Advantage in training Neural networks with more hidden layers\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_15.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Boxcar function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs_nn/fig01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A unit with single input and sigmoid activation\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_17.jpg)\n",
    "\n",
    "- Unit outputs for different values of *b* and *w*\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_18.jpg)\n",
    "\n",
    "- Combine two units with a hidden layer\n",
    "\n",
    "  ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_19.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal approximation theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f_\\theta(x) = g(x)$\n",
    "  - *x* is the input data\n",
    "  - $\\theta$ are the NN weights\n",
    "  - $g(x)$ : collection of input samples and labels\n",
    "\n",
    "- Any continuous function can be approximated to an arbitrary degree of accuracy by a feedforward NN\n",
    "   - with at least one hidden layer with a finite number of units and a non-linear activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Find parameters $\\theta$ such that $f_\\theta(x)$ will be the best approximation of $g(x)$\n",
    " - Train NN using **mean square error (MSE)** cost function\n",
    "     - measures the difference (**error**) between the network output and the training data labels $t^{(i)}$ of all the training samples $x^{(i)}$\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/157.png)\n",
    "\n",
    " - Gradient Descent (GD)\n",
    "   - Compute the derivative (gradient) of $J(\\theta)$ with respect to all the network weights\n",
    "       - gives indication of how   $J(\\theta)$ changes with respect to each weight\n",
    "   - Uses this information to update the weights to minimize  $J(\\theta)$ in future iterations\n",
    "   - Goal is to gradually reach the **global minimum** of the cost function, where the gradient is 0\n",
    "  \n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_20.jpg)\n",
    "\n",
    "- Initialize the network weights, $\\theta$, with random values\n",
    "- Repeat the following steps until the cost function, $J(\\theta)$, falls below a certain threshold\n",
    "    - **Forward pass**\n",
    "        - Compute the MSE   $J(\\theta)$ cost function for all the samples of the training set\n",
    "    - **Backward pass**\n",
    "        - Compute the partial derivatives (gradients) of  $J(\\theta)$ with respect to all the network weights $\\theta_j$ using the chain rule\n",
    "     \n",
    "          ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/166.png)\n",
    "\n",
    "        - Use the gradient values to update each of the network weights, where $\\eta$ is the **learning rate**, which determines the step size at which the optimizer updates the weights during training\n",
    "\n",
    "      ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/170.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Gradient descent may converge to a local minimum\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_02_21.jpg)\n",
    "\n",
    " - Adjust the current weight update with the values of the previous weight updates\n",
    "     - if the weight update in the previous step was big, it will also increase the weight update in the next step (**momentum**)\n",
    "  \n",
    " - Weight update rule\n",
    "\n",
    "    ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/183.png)\n",
    "   \n",
    " - During step *t* of the training process\n",
    "     - First, calculate the current weight update value $v_t$ by also including the velocity of the previous update $v_{t-1}$. $\\mu$ is the hyperparameter in the range [0,1], called the *momentum* rate\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/186.png)\n",
    "     \n",
    " -  Then, do the actual weight update  \n",
    "    \n",
    " ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/188.png)\n",
    "\n",
    "\n",
    " - Adative learning rate algorithm **Adam**\n",
    "     - Calculates individual and adaptive learning rates for every weight, based on previous weight updates (momentum) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Batch gradient descent**\n",
    "    - As described above\n",
    "    - Accumulates the error across all the training samples and performs a single weight update \n",
    "- **Stochastic** (or **online**) **gradient descent**\n",
    "    - Updates the weights after every training sample \n",
    "- **Mini-batch gradient descent**\n",
    "    - Accumulates the error over batches of $k$ samples and performs one weight update after each mini-batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - xor Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-2 * x)) / (1.0 + np.exp(-2 * x))\n",
    "\n",
    "\n",
    "def tanh_derivative(x):\n",
    "    return (1 + tanh(x)) * (1 - tanh(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    # net_arch consists of a list of integers, indicating\n",
    "    # the number of neurons in each layer\n",
    "    def __init__(self, net_arch):\n",
    "        self.activation_func = tanh\n",
    "        self.activation_derivative = tanh_derivative\n",
    "        self.layers = len(net_arch)\n",
    "        self.steps_per_epoch = 1000\n",
    "        self.net_arch = net_arch\n",
    "\n",
    "        # initialize the weights with random values in the range (-1,1)\n",
    "        self.weights = []\n",
    "        for layer in range(len(net_arch) - 1):\n",
    "            w = 2 * np.random.rand(net_arch[layer] + 1, net_arch[layer + 1]) - 1\n",
    "            self.weights.append(w)\n",
    "\n",
    "    def fit(self, data, labels, learning_rate=0.1, epochs=10):\n",
    "        \"\"\"\n",
    "        :param data: data is the set of all possible pairs of booleans\n",
    "                     True or False indicated by the integers 1 or 0\n",
    "                     labels is the result of the logical operation 'xor'\n",
    "                     on each of those input pairs\n",
    "        :param labels: array of 0/1 for each datum\n",
    "        \"\"\"\n",
    "\n",
    "        # Add bias units to the input layer\n",
    "        bias = np.ones((1, data.shape[0]))\n",
    "        input_data = np.concatenate((bias.T, data), axis=1)\n",
    "\n",
    "        for k in range(epochs * self.steps_per_epoch):\n",
    "            if k % self.steps_per_epoch == 0:\n",
    "                # print ('epochs:', k/self.steps_per_epoch)\n",
    "                print('epochs: {}'.format(k / self.steps_per_epoch))\n",
    "                for s in data:\n",
    "                    print(s, nn.predict(s))\n",
    "\n",
    "            sample = np.random.randint(data.shape[0])\n",
    "            y = [input_data[sample]]\n",
    "\n",
    "            for i in range(len(self.weights) - 1):\n",
    "                activation = np.dot(y[i], self.weights[i])\n",
    "                activation_f = self.activation_func(activation)\n",
    "                # add the bias for the next layer\n",
    "                activation_f = np.concatenate((np.ones(1), np.array(activation_f)))\n",
    "                y.append(activation_f)\n",
    "\n",
    "            # last layer\n",
    "            activation = np.dot(y[-1], self.weights[-1])\n",
    "            activation_f = self.activation_func(activation)\n",
    "            y.append(activation_f)\n",
    "\n",
    "            # error for the output layer\n",
    "            error = y[-1] - labels[sample]\n",
    "            delta_vec = [error * self.activation_derivative(y[-1])]\n",
    "\n",
    "            # we need to begin from the back from the next to last layer\n",
    "            for i in range(self.layers - 2, 0, -1):\n",
    "                error = delta_vec[-1].dot(self.weights[i][1:].T)\n",
    "                error = error * self.activation_derivative(y[i][1:])\n",
    "                delta_vec.append(error)\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            delta_vec.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation\n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Update the weight using the weight update formula\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = y[i].reshape(1, nn.net_arch[i] + 1)\n",
    "\n",
    "                delta = delta_vec[i].reshape(1, nn.net_arch[i + 1])\n",
    "                self.weights[i] -= learning_rate * layer.T.dot(delta)\n",
    "\n",
    "    def predict(self, x):\n",
    "        val = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "        for i in range(0, len(self.weights)):\n",
    "            val = self.activation_func(np.dot(val, self.weights[i]))\n",
    "            val = np.concatenate((np.ones(1).T, np.array(val)))\n",
    "\n",
    "        return val[1]\n",
    "\n",
    "    def plot_decision_regions(self, X, y, points=200):\n",
    "        markers = ('o', '^')\n",
    "        colors = ('red', 'blue')\n",
    "        cmap = ListedColormap(colors)\n",
    "\n",
    "        x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "        x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "\n",
    "        resolution = max(x1_max - x1_min, x2_max - x2_min) / float(points)\n",
    "\n",
    "        xx1, xx2 = np.meshgrid(np.arange(x1_min,\n",
    "                                         x1_max,\n",
    "                                         resolution),\n",
    "                               np.arange(x2_min, x2_max, resolution))\n",
    "        input = np.array([xx1.ravel(), xx2.ravel()]).T\n",
    "        Z = np.empty(0)\n",
    "        for i in range(input.shape[0]):\n",
    "            val = nn.predict(np.array(input[i]))\n",
    "            if val < 0.5:\n",
    "                val = 0\n",
    "            if val >= 0.5:\n",
    "                val = 1\n",
    "            Z = np.append(Z, val)\n",
    "\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "\n",
    "        plt.pcolormesh(xx1, xx2, Z, cmap=cmap)\n",
    "        plt.xlim(xx1.min(), xx1.max())\n",
    "        plt.ylim(xx2.min(), xx2.max())\n",
    "        # plot all samples\n",
    "\n",
    "        classes = [\"False\", \"True\"]\n",
    "\n",
    "        for idx, cl in enumerate(np.unique(y)):\n",
    "            plt.scatter(x=X[y == cl, 0],\n",
    "                        y=X[y == cl, 1],\n",
    "                        alpha=1.0,\n",
    "                        c=colors[idx],\n",
    "                        edgecolors='black',\n",
    "                        marker=markers[idx],\n",
    "                        s=80,\n",
    "                        label=classes[idx])\n",
    "\n",
    "        plt.xlabel('x1')\n",
    "        plt.ylabel('x2')\n",
    "        plt.legend(loc='upper left')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0.0\n",
      "[0 0] 0.31634987228520156\n",
      "[0 1] 0.38455314510086014\n",
      "[1 0] 0.49960366414001517\n",
      "[1 1] 0.5470092417007291\n",
      "epochs: 1.0\n",
      "[0 0] 0.10110562119575028\n",
      "[0 1] 0.4983062530300435\n",
      "[1 0] 0.5483740117095983\n",
      "[1 1] 0.635812878112665\n",
      "epochs: 2.0\n",
      "[0 0] 0.07164948329787507\n",
      "[0 1] 0.861075813281495\n",
      "[1 0] 0.8502850626450226\n",
      "[1 1] 0.07158530421971575\n",
      "epochs: 3.0\n",
      "[0 0] 0.017586567899253898\n",
      "[0 1] 0.9666637734019891\n",
      "[1 0] 0.9651222166853127\n",
      "[1 1] 0.011468668342141863\n",
      "epochs: 4.0\n",
      "[0 0] -0.0017118993569209561\n",
      "[0 1] 0.9815292663780985\n",
      "[1 0] 0.9828812324283912\n",
      "[1 1] -0.00030370918696389856\n",
      "epochs: 5.0\n",
      "[0 0] 0.0026985374081322524\n",
      "[0 1] 0.9885083594808965\n",
      "[1 0] 0.9891298042443863\n",
      "[1 1] 0.015552778145750895\n",
      "epochs: 6.0\n",
      "[0 0] 0.005625211435814411\n",
      "[0 1] 0.9922099656276941\n",
      "[1 0] 0.9915443580479176\n",
      "[1 1] 0.01694332658239157\n",
      "epochs: 7.0\n",
      "[0 0] 0.0019544398675395446\n",
      "[0 1] 0.9934850143000605\n",
      "[1 0] 0.9934672674082785\n",
      "[1 1] 0.0007886110283738284\n",
      "epochs: 8.0\n",
      "[0 0] 0.0036493566842653565\n",
      "[0 1] 0.9950489745378326\n",
      "[1 0] 0.9943655959618727\n",
      "[1 1] 0.003149359618501733\n",
      "epochs: 9.0\n",
      "[0 0] 0.002330494423308202\n",
      "[0 1] 0.9958242355532402\n",
      "[1 0] 0.9953474336963716\n",
      "[1 1] 0.006106035948579757\n",
      "Final prediction\n",
      "[0 0] 0.003032173692499404\n",
      "[0 1] 0.9963860761357731\n",
      "[1 0] 0.9959034563937058\n",
      "[1 1] 0.000638644921756766\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# Initialize the NeuralNetwork with 2 input, 2 hidden, and 1 output neurons\n",
    "nn = NeuralNetwork([2, 2, 1])\n",
    "\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "nn.fit(X, y, epochs=10)\n",
    "\n",
    "print(\"Final prediction\")\n",
    "for s in X:\n",
    "    print(s, nn.predict(s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nn.plot_decision_regions(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "  - Python Deep Learning, Third Edition, Ivan Vasilev, Packt Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
