{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21d9c515-4871-429f-bef6-87f068f52349",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Deep Learning Fundamentals\"\n",
    "format: html\n",
    "page-layout: full\n",
    "code-line-numbers: true\n",
    "code-block-border: true\n",
    "toc: true\n",
    "toc-location: left\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5ae4be-2518-4a37-8de7-3c82be869327",
   "metadata": {},
   "source": [
    "- Information is processed in hierarchical layers\n",
    "    - understand representations and features from data\n",
    "- Multilayer perceptron (MLP)\n",
    "    - A NN with feedforward propagation, fully connected layers, and at least one hidden layer\n",
    "- Convolutional neural network (CNN)\n",
    "    - A feedforward NN with several types of special layers\n",
    "    - apply filters to the input image (or sound) by sliding the filter all across the incoming signal\n",
    "- Recurrent neural network (RNN)\n",
    "    - Has an internal state or memory based on all, or part of, the input data already fed to the network\n",
    "    - Output is a combination of its internal state and the latest input sample\n",
    "    - Good for tasks that work on sequential data, e.g., text or time series data\n",
    "- Transformer\n",
    "    - Suited for sequential data\n",
    "    - Uses a technique called **attention**, allows direct simultaneous access to all elements of the input sequence\n",
    "    - Superseded RNNs in may tasks\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0547c9-3496-4bb0-b8a0-1beddad868db",
   "metadata": {},
   "source": [
    "# Activation Functions - Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7cf27a-0426-4d9d-a692-8174f5fc6756",
   "metadata": {},
   "source": [
    " - Assume backpropagation to train a MLP with multiple hidden layers and logistic sigmoid function at each layer\n",
    " - $\\sigma(x) = 1/(1 + e^{-x})$\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_03_4.jpg)\n",
    "\n",
    "- Forward phase\n",
    "  - Output of the first sigmoid layer fall in the range (0,1)\n",
    "  - For consecutive layers, the range becomes narrower\n",
    "     - After three layers, for example, the activation converges to around 0.66 regardless of the input value\n",
    "  - Acts as a eraser of any information coming from the preceding layers\n",
    "\n",
    "- Backward phase\n",
    "  - Derivative of the sigmoid function has a significant value in a narrow interval centered around 0\n",
    "      - converges to 0 in all other cases\n",
    "  - In networks with many layers, the derivative would likely converge to 0 when propogated to the first layers\n",
    "      - thus, the weights cannot be updated in a meaningful way    \n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cf63f0-aad6-40d8-8271-cc5f7b0d4c31",
   "metadata": {},
   "source": [
    "- **ReLU** activation function\n",
    "  - solves the vanishing gradients problem\n",
    "    \n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_03_5.jpg)\n",
    "\n",
    "- **Idempotent**\n",
    "    - Value doesn't change through any number of layers\n",
    "        - ReLU(2) = 2, ReLU(ReLU(2)) = 2, ...\n",
    "- Derivative is either 0 or 1, regardless of the backpropagated value    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df64a9bc-ce61-4b12-b59b-8e70a8484225",
   "metadata": {},
   "source": [
    "- Problem with **ReLU**\n",
    "    - During training, when network weights are being updated, some of the ReLU units may always receive inputs smaller than 0, and hence always output 0 as well\n",
    "    - This is known as **dying ReLUs**\n",
    "- **Leaky ReLU**\n",
    "    - When $x \\lt 0$, outputs $x$ multiplied by a constant $\\alpha$ ($0 \\lt \\alpha \\lt 1$)\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_03_6.jpg)\n",
    " \n",
    "-  **Parametric ReLU**\n",
    "    - Same as leaky ReLU, but $\\alpha$ is tunable and adjusted during training\n",
    " \n",
    "- **Exponential linear unit (ELU)**\n",
    "    - When $x \\lt 0$, outputs $\\alpha(e^x - 1)$, where $\\alpha$ is a tunable parameter\n",
    "    - For example, for $\\alpha = 0.2$\n",
    "    - \n",
    " ![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_03_7.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74538355-bd47-4a0a-9a3a-e43d7a1c4b4c",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae5355-54ec-436a-8119-b108f39513bc",
   "metadata": {},
   "source": [
    " - Activation function of the output layer in classification problems\n",
    " - Output of the final network layer, $z = (z_1, z_2, ..., z_n)$\n",
    "     - Each of the $n$ elements represents one of $n$ classes to which the input sample might belong\n",
    " - For network prediction, the index $i$ of the highest value $z_i$ is assigned as the class of that input sample\n",
    " - For interpreting network output as probability distribution, use the softmax activation:\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/242.png)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5163a900-d859-40f1-a874-1a01b81403e1",
   "metadata": {},
   "source": [
    "# DNN Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956f46a-46d5-48ff-a06a-543b2967e9ec",
   "metadata": {},
   "source": [
    "- The NN may learn to approximate the noise of the target function rather than its useful components\n",
    "- Example\n",
    "    - Training data with mostly images of red cars, NN to classify whether car or not\n",
    "    - Can associate color red with the car rather than the shape\n",
    "    - May fail to classify a green car since the color doesn't match\n",
    "- Avoid overfitting using **regularization** techniques\n",
    "- Some Techniues to apply to input data before feeding into NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d11f695-971e-4858-9c95-68b90ded65e6",
   "metadata": {},
   "source": [
    "### Min-max normalization\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/253.png)\n",
    "\n",
    "\n",
    " - Scales all input to [0, 1] range\n",
    " - Easy to implement\n",
    " - However, outliers with large value may drive all normalized values toward 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcfdaf7-d71e-4c3d-8e0b-46eedc0fe8ef",
   "metadata": {},
   "source": [
    "### Z-score normalization\n",
    "\n",
    "$$\n",
    " z = \\frac{x - \\mu}{\\sigma} \\,\n",
    "$$\n",
    "\n",
    "- Handles outliers better than min-max\n",
    "- Maintains the dataset's mean values close to 0 and standard deviation close to 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f49ec4-ab2f-4f45-b3f5-30c39a24590a",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    " - Artificially increase the size of the training set\n",
    " - apply random modifications to the training samples before feeding to the network\n",
    " - for images, rotation, skew, scaling, etc. can be used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2171d1-540d-4784-b6c8-d5398d55d7df",
   "metadata": {},
   "source": [
    "**Regularization techniques with the DNN structure**\n",
    "\n",
    "### Dropout\n",
    "\n",
    " - Randomly and periodically remove some of the units of a layer from the network\n",
    " - During a training mini-batch, each unit has a probability, *p*, of being dropped\n",
    " - Ensures no unit relies too much on other units\n",
    " - Applied during the training phase\n",
    " - All units participate during the inference phase\n",
    "\n",
    "![](https://static.packt-cdn.com/products/9781837638505/graphics/image/B19627_03_9.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcaaed1-7f42-4560-956b-a90cc08e6905",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "\n",
    " - Normalizes the outputs of the hidden layer for each mini-batch, thus maintaining its mean activation value close to 0 and its standard deviation close to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371102c-1817-4b5a-901e-806a0f964bee",
   "metadata": {},
   "source": [
    "# Example - Classifying Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a97a8d3-fe15-4d99-99ee-5a84de07884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d228f2b-e506-46aa-8645-5f685fd0db9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    transform=Compose(\n",
    "        [ToTensor(),\n",
    "         Lambda(lambda x: torch.flatten(x))]),\n",
    "    download=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6208eb8f-fbe7-4629-bffc-54e3743cc609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 28, 28])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122edbd6-772a-4ffa-9c00-37fe338ef935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6224b8fb-697b-4918-bbb8-18e3cb671427",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = datasets.MNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    transform=Compose(\n",
    "        [ToTensor(),\n",
    "         Lambda(lambda x: torch.flatten(x))]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44443efa-744e-49f5-8560-f755499b371a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 28, 28])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f968224-dbed-4034-ab90-d5aff3d20088",
   "metadata": {},
   "source": [
    " - ToTensor() transformation converts *numpy* images to *PyTorch* tensors and normalizes them to [0,1] range\n",
    " - *torch.flatten()* transform flattens two-dimensional 28 x 28 images to a one-dimentional 784 tensor to feed to the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b3b209-f0cb-4acb-9eb0-685b44b87347",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=100,\n",
    "    shuffle=True)\n",
    "\n",
    "validation_loader = DataLoader(\n",
    "    dataset=validation_data,\n",
    "    batch_size=100,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367f3be-2792-43be-b565-32312cc4968c",
   "metadata": {},
   "source": [
    " - *DataLoader* instance creates mini-bathces and shuffles the data randomly\n",
    " - They are also *iterators*, which supply mini-batches one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "647b19f7-e9f2-41dd-82dd-0756f2992fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  A MLP with one hidden layer\n",
    "\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "hidden_units = 100\n",
    "classes = 10\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(28 * 28, hidden_units),\n",
    "    torch.nn.BatchNorm1d(hidden_units),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(hidden_units, classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84868365-f676-42aa-834a-89d501f5127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cross-entropy loss and the Adam optimizer\n",
    "\n",
    "cost_func = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0944d3e0-4485-4737-89ab-6e5339ce0b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model for a single epoch\n",
    "\n",
    "def train_model(model, cost_function, optimizer, data_loader):\n",
    "    # send the model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    current_acc = 0\n",
    "\n",
    "    # iterate over the training data\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # send the input/labels to the GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            loss = cost_function(outputs, labels)\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # statistics\n",
    "        current_loss += loss.item() * inputs.size(0)\n",
    "        current_acc += torch.sum(predictions == labels.data)\n",
    "\n",
    "    total_loss = current_loss / len(data_loader.dataset)\n",
    "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
    "\n",
    "    print('Train Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf20c75-572f-4d95-8c16-dcfc2f156128",
   "metadata": {},
   "source": [
    " - iterates over all mini-batches provided by train_loader\n",
    " - For each mini-batch, optimizer.zero_grad() resets the gradients from the previous iteration\n",
    " - Then, we initiate the forward and backward passes, and finally the weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5d6d4eb-c3e8-4bf2-b3af-c1b7d526fcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, cost_function, data_loader):\n",
    "    # send the model to the GPU\n",
    "    model.to(device)\n",
    "\n",
    "    # set model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    current_loss = 0.0\n",
    "    current_acc = 0\n",
    "\n",
    "    # iterate over  the validation data\n",
    "    for i, (inputs, labels) in enumerate(data_loader):\n",
    "        # send the input/labels to the GPU\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            loss = cost_function(outputs, labels)\n",
    "\n",
    "        # statistics\n",
    "        current_loss += loss.item() * inputs.size(0)\n",
    "        current_acc += torch.sum(predictions == labels.data)\n",
    "\n",
    "    total_loss = current_loss / len(data_loader.dataset)\n",
    "    total_acc = current_acc.double() / len(data_loader.dataset)\n",
    "\n",
    "    print('Test Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, total_acc))\n",
    "\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f6bde-269e-41cd-a97d-cacf41e09375",
   "metadata": {},
   "source": [
    " - Batch normalization and dropout layers are not used in evaluation (only in training), so model.eval() turns them off\n",
    " - We iterate over the validation set, initiate a forward pass, and aggregate the validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9386aba-efa0-420c-aa20-7c5fa18cafe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3272; Accuracy: 0.9175\n",
      "Train Loss: 0.1421; Accuracy: 0.9604\n",
      "Train Loss: 0.0999; Accuracy: 0.9721\n",
      "Train Loss: 0.0760; Accuracy: 0.9790\n",
      "Train Loss: 0.0611; Accuracy: 0.9828\n",
      "Train Loss: 0.0496; Accuracy: 0.9861\n",
      "Train Loss: 0.0422; Accuracy: 0.9879\n",
      "Train Loss: 0.0359; Accuracy: 0.9898\n",
      "Train Loss: 0.0310; Accuracy: 0.9914\n",
      "Train Loss: 0.0265; Accuracy: 0.9928\n",
      "Train Loss: 0.0226; Accuracy: 0.9937\n",
      "Train Loss: 0.0200; Accuracy: 0.9947\n",
      "Train Loss: 0.0183; Accuracy: 0.9948\n",
      "Train Loss: 0.0175; Accuracy: 0.9952\n",
      "Train Loss: 0.0160; Accuracy: 0.9957\n",
      "Train Loss: 0.0130; Accuracy: 0.9967\n",
      "Train Loss: 0.0130; Accuracy: 0.9964\n",
      "Train Loss: 0.0105; Accuracy: 0.9975\n",
      "Train Loss: 0.0104; Accuracy: 0.9973\n",
      "Train Loss: 0.0104; Accuracy: 0.9973\n"
     ]
    }
   ],
   "source": [
    "# Run the training for 20 epochs\n",
    "\n",
    "for epoch in range(20):\n",
    "    train_model(model, cost_func, optimizer, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1666e37f-aa66-4cc2-9fee-9eecab4c7b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0882; Accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.08819967041490599, tensor(0.9774, dtype=torch.float64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "test_model(model, cost_func, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f788ff-b759-454a-b139-415240ba2f7b",
   "metadata": {},
   "source": [
    "# References\n",
    "  - Python Deep Learning, Third Edition, Ivan Vasilev, Packt Publishing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
