{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb18e2-3ce2-49db-8593-7480e7152f1f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Planning with Uncertainty (Part2 - Reinforcement Learning)\"\n",
    "format: html\n",
    "page-layout: full\n",
    "code-line-numbers: true\n",
    "code-block-border: true\n",
    "toc: true\n",
    "toc-location: left\n",
    "number-sections: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78ac7a-e817-472a-8844-4a2a8da2fb31",
   "metadata": {},
   "source": [
    "# Representing Agents and Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f04882c-e8cb-4dec-b053-9d9ac4af4e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81789764-d0a8-488c-8aa6-42b4533f0f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip(prob):\n",
    "    \"\"\"return true with probability prob\"\"\"\n",
    "    return random.random() < prob\n",
    "\n",
    "def argmaxall(gen):\n",
    "    \"\"\"gen is a generator of (element,value) pairs, where value is a real.\n",
    "    argmaxall returns a list of all of the elements with maximal value.\n",
    "    \"\"\"\n",
    "    maxv = -math.inf       # negative infinity\n",
    "    maxvals = []      # list of maximal elements\n",
    "    for (e,v) in gen:\n",
    "        if v>maxv:\n",
    "            maxvals,maxv = [e], v\n",
    "        elif v==maxv:\n",
    "            maxvals.append(e)\n",
    "    return maxvals\n",
    "\n",
    "def argmaxe(gen):\n",
    "    \"\"\"gen is a generator of (element,value) pairs, where value is a real.\n",
    "    argmaxe returns an element with maximal value.\n",
    "    If there are multiple elements with the max value, one is returned at random.\n",
    "    \"\"\"\n",
    "    return random.choice(argmaxall(gen))\n",
    "\n",
    "def argmax(lst):\n",
    "    \"\"\"returns maximum index in a list\"\"\"\n",
    "    return argmaxe(enumerate(lst))\n",
    "\n",
    "def argmaxd(dct):\n",
    "   \"\"\"returns the arg max of a dictionary dct\"\"\"\n",
    "   return argmaxe(dct.items())\n",
    "\n",
    "def select_from_dist(item_prob_dist):\n",
    "    \"\"\" returns a value from a distribution.\n",
    "    item_prob_dist is an item:probability dictionary, where the\n",
    "        probabilities sum to 1.\n",
    "    returns an item chosen in proportion to its probability\n",
    "    \"\"\"\n",
    "    ranreal = random.random()\n",
    "    for (it,prob) in item_prob_dist.items():\n",
    "        if ranreal < prob:\n",
    "            return it\n",
    "        else:\n",
    "            ranreal -= prob\n",
    "    raise RuntimeError(f\"{item_prob_dist} is not a probability distribution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3711c11-6a4a-49ec-b443-cbda583f2c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Displayable(object):\n",
    "    \"\"\"Class that uses 'display'.\n",
    "    The amount of detail is controlled by max_display_level\n",
    "    \"\"\"\n",
    "    max_display_level = 1   # can be overridden in subclasses or instances\n",
    "\n",
    "    def display(self,level,*args,**nargs):\n",
    "        \"\"\"print the arguments if level is less than or equal to the\n",
    "        current max_display_level.\n",
    "        level is an integer.\n",
    "        the other arguments are whatever arguments print can take.\n",
    "        \"\"\"\n",
    "        if level <= self.max_display_level:\n",
    "            print(*args, **nargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0cf74e-2451-422e-a2cb-3be0b67ff23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Displayable):\n",
    "\n",
    "    def initial_action(self, percept):\n",
    "        \"\"\"return the initial action.\"\"\"\n",
    "        return self.select_action(percept)   # same as select_action\n",
    "\n",
    "    def select_action(self, percept):\n",
    "        \"\"\"return the next action (and update internal state) given percept\n",
    "        percept is variable:value dictionary\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"go\")   # abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ad9b124-503c-4071-854d-249a06b1ec0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment(Displayable):\n",
    "    def initial_percept(self):\n",
    "        \"\"\"returns the initial percept for the agent\"\"\"\n",
    "        raise NotImplementedError(\"initial_percept\")   # abstract method\n",
    "\n",
    "    def do(self, action):\n",
    "        \"\"\"does the action in the environment\n",
    "        returns the next percept \"\"\"\n",
    "        raise NotImplementedError(\"Environment.do\")   # abstract method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f028c8d3-3f0c-44c4-9f25-3982bbd62078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_env(Environment):\n",
    "    def __init__(self, name, actions, state):\n",
    "        \"\"\"creates an environment given name, list of actions, and initial state\"\"\"\n",
    "        self.name = name         # the role for an agent \n",
    "        self.actions = actions   # list of all actions\n",
    "        self.state = state       # initial state\n",
    "        self.reward = None       # last reward\n",
    "\n",
    "    # must implement do(action)->(reward,state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3db6561-d146-4d77-a6fa-c8b5a5c02fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RL_agent(Agent):\n",
    "    \"\"\"An RL_Agent \n",
    "    has percepts (s, r) for some state s and real reward r\n",
    "    \"\"\"\n",
    "    def __init__(self, actions):\n",
    "       self.actions = actions\n",
    "\n",
    "    def initial_action(self, env_state):\n",
    "        \"\"\"return the initial action, and remember the state and action\n",
    "        Act randomly initially\n",
    "        Could be overridden to initialize data structures (as the agent now knows about one state)\n",
    "        \"\"\"\n",
    "        self.state = env_state\n",
    "        self.action = random.choice(self.actions)\n",
    "        return self.action\n",
    "\n",
    "    def select_action(self, reward, state):\n",
    "        \"\"\" \n",
    "        Select the action given the reward and next state\n",
    "        Remember the action in self.action\n",
    "        This implements \"Act randomly\" and should  be overridden!\n",
    "        \"\"\"\n",
    "        self.reward = reward\n",
    "        self.action = random.choice(self.actions)\n",
    "        return self.action\n",
    "\n",
    "    def v(self, state):\n",
    "        \"v needed for GUI; an agent must also implement q()\"\n",
    "        return max(self.q(state,a) for a in self.actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbbf78-423b-4f60-88d9-ecddf0ca1a8c",
   "metadata": {},
   "source": [
    "## Simulating Environment-Agent interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14d937a9-5489-4a72-94e8-eb652f4e9475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simulate(Displayable):\n",
    "    \"\"\"simulate the interaction between the agent and the environment\n",
    "    for n time steps.\n",
    "    Returns a pair of the agent state and the environment state.\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, environment):\n",
    "        self.agent = agent\n",
    "        self.env = environment\n",
    "        self.reward_history = []  # for plotting\n",
    "        self.step = 0\n",
    "        self.sum_rewards = 0\n",
    "\n",
    "    def start(self):\n",
    "        self.action = self.agent.initial_action(self.env.state)\n",
    "        return self\n",
    "\n",
    "    def go(self, n):\n",
    "        for i in range(n):\n",
    "            self.step += 1\n",
    "            (reward,state) = self.env.do(self.action)\n",
    "            self.display(2,f\"step={self.step} reward={reward}, state={state}\")\n",
    "            self.sum_rewards += reward\n",
    "            self.reward_history.append(reward)\n",
    "            self.action = self.agent.select_action(reward,state)\n",
    "            self.display(2,f\"      action={self.action}\")\n",
    "        return self\n",
    "\n",
    "    def plot(self, label=None, step_size=None, xscale='linear'):\n",
    "        \"\"\"\n",
    "        plots the rewards history in the simulation\n",
    "        label is the label for the plot\n",
    "        step_size is the number of steps between each point plotted\n",
    "        xscale is 'log' or 'linear'\n",
    "\n",
    "        returns sum of rewards\n",
    "        \"\"\"\n",
    "        if step_size is None: #for long simulations (> 999), only plot some points\n",
    "            step_size = max(1,len(self.reward_history)//500)\n",
    "        if label is None:\n",
    "            label = self.agent.method\n",
    "        plt.ion()\n",
    "        plt.xscale(xscale)\n",
    "        plt.xlabel(\"step\")\n",
    "        plt.ylabel(\"Sum of rewards\")\n",
    "        sum_history, sum_rewards = acc_rews(self.reward_history, step_size)\n",
    "        plt.plot(range(0,len(self.reward_history),step_size), sum_history, label=label)\n",
    "        plt.legend()\n",
    "        plt.draw()\n",
    "        return sum_rewards\n",
    "\n",
    "def acc_rews(rews,step_size):\n",
    "    \"\"\"returns the rolling sum of the values, sampled each step_size, and the sum\n",
    "    \"\"\"\n",
    "    acc = []\n",
    "    sumr = 0; i=0\n",
    "    for e in rews:\n",
    "       sumr += e\n",
    "       i += 1\n",
    "       if (i%step_size == 0): \n",
    "           acc.append(sumr)\n",
    "    return acc, sumr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f915798-b85e-4aab-9431-cdad2ea26578",
   "metadata": {},
   "source": [
    "## Party Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7a2a2b9-f8b4-4e56-8279-9c3d8395c071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Party_env(RL_env):\n",
    "    def __init__(self):\n",
    "        RL_env.__init__(self, \"Party Decision\", [\"party\", \"relax\"], \"healthy\")\n",
    "\n",
    "    def do(self, action):\n",
    "        \"\"\"updates the state based on the agent doing action.\n",
    "        returns reward,state\n",
    "        \"\"\"\n",
    "        if self.state==\"healthy\":\n",
    "            if action==\"party\":\n",
    "                self.state = \"healthy\" if flip(0.7) else \"sick\"\n",
    "                self.reward = 10\n",
    "            else:  # action==\"relax\"\n",
    "                self.state = \"healthy\" if flip(0.95) else \"sick\"\n",
    "                self.reward = 7\n",
    "        else:  # self.state==\"sick\"\n",
    "            if action==\"party\":\n",
    "                self.state = \"healthy\" if flip(0.1) else \"sick\"\n",
    "                self.reward = 2\n",
    "            else:\n",
    "                self.state = \"healthy\" if flip(0.5) else \"sick\"\n",
    "                self.reward = 0\n",
    "                \n",
    "        return self.reward, self.state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518ea2f-7e6c-4a19-9c98-60f9c3ff4c91",
   "metadata": {},
   "source": [
    "## Environment from a Problem Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc89a9-2b6d-4439-b623-92644ab05145",
   "metadata": {},
   "source": [
    " - Takes a *ProblemDomain* (from MDP examples)\n",
    " - Constructs an environment for reinforcement learners\n",
    " - Representation of MDP does not contain enough information to sumulate a system\n",
    "     - loses any dependency between the rewards and the resulting state\n",
    "     - only represents the expected value of rewards\n",
    "     - not how they are distributed\n",
    " - *ProblemDomain* class defines the *result* method to map states and actions into distributions over (reward, state) pairs   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c523d19-ef4e-4ce7-af16-ee5e1cb1cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_from_ProblemDomain(RL_env):\n",
    "    def __init__(self, prob_dom):\n",
    "        RL_env.__init__(self, prob_dom.title, prob_dom.actions, prob_dom.state)\n",
    "        self.problem_domain = prob_dom\n",
    "        self.state = prob_dom.state\n",
    "        self.x_dim = prob_dom.x_dim\n",
    "        self.y_dim = prob_dom.y_dim \n",
    "        self.offsets = prob_dom.offsets\n",
    "        self.state2pos = self.problem_domain.state2pos\n",
    "        self.state2goal = self.problem_domain.state2goal\n",
    "        self.pos2state = self.problem_domain.pos2state\n",
    "        \n",
    "    def do(self, action):\n",
    "        \"\"\"updates the state based on the agent doing action.\n",
    "        returns state,reward\n",
    "        \"\"\"\n",
    "        (self.reward, self.state) = select_from_dist(\n",
    "            self.problem_domain.result(self.state, action))\n",
    "        \n",
    "        self.problem_domain.state = self.state\n",
    "        self.display(2,f\"do({action} -> ({self.reward}, {self.state})\")\n",
    "        return (self.reward, self.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e1e33-8a19-4ad8-85e6-e713a2cb9d1f",
   "metadata": {},
   "source": [
    "## Monster Game Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfe526-3854-432d-9dd2-4c289db4d928",
   "metadata": {},
   "source": [
    "![](https://artint.info/3e/html/x145.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c760c86-b187-451e-822a-558e82683f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Monster_game_env(RL_env):\n",
    "    x_dim = 5\n",
    "    y_dim = 5\n",
    "\n",
    "    vwalls = [(0,3), (0,4), (1,4)]  # vertical walls right of these locations\n",
    "    hwalls = [] # not implemented\n",
    "    crashed_reward = -1\n",
    "    \n",
    "    prize_locs = [(0,0), (0,4), (4,0), (4,4)]\n",
    "    prize_apears_prob = 0.3\n",
    "    prize_reward = 10\n",
    "\n",
    "    monster_locs = [(0,1), (1,1), (2,3), (3,1), (4,2)]\n",
    "    monster_appears_prob = 0.4\n",
    "    monster_reward_when_damaged = -10\n",
    "    repair_stations = [(1,4)]\n",
    "\n",
    "    actions = [\"up\",\"down\",\"left\",\"right\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        # State:\n",
    "        self.x = 2\n",
    "        self.y = 2\n",
    "        self.damaged = False\n",
    "        self.prize = None\n",
    "        # Statistics\n",
    "        self.number_steps = 0\n",
    "        self.accumulated_rewards = 0   # sum of rewards received\n",
    "        self.min_accumulated_rewards = 0\n",
    "        self.min_step = 0\n",
    "        self.zero_crossing = 0\n",
    "        RL_env.__init__(self, \"Monster Game\", self.actions, (self.x, self.y, self.damaged, self.prize))\n",
    "        self.display(2,\"\",\"Step\",\"Tot Rew\",\"Ave Rew\",sep=\"\\t\")\n",
    "\n",
    "    def do(self,action):\n",
    "        \"\"\"updates the state based on the agent doing action.\n",
    "        returns reward,state\n",
    "        \"\"\"\n",
    "        assert action in self.actions, f\"Monster game, unknown action: {action}\"\n",
    "        \n",
    "        self.reward = 0.0\n",
    "        \n",
    "        # A prize can appear:\n",
    "        if self.prize is None and flip(self.prize_apears_prob):\n",
    "                self.prize = random.choice(self.prize_locs)\n",
    "        \n",
    "        # Actions can be noisy\n",
    "        if flip(0.4):\n",
    "            actual_direction = random.choice(self.actions)\n",
    "        else:\n",
    "            actual_direction = action\n",
    "        \n",
    "        # Modeling the actions given the actual direction\n",
    "        if actual_direction == \"right\":\n",
    "            if self.x==self.x_dim-1 or (self.x,self.y) in self.vwalls:\n",
    "                self.reward += self.crashed_reward\n",
    "            else:\n",
    "                self.x += 1\n",
    "        elif actual_direction == \"left\":\n",
    "            if self.x==0 or (self.x-1,self.y) in self.vwalls:\n",
    "                self.reward += self.crashed_reward\n",
    "            else:\n",
    "                self.x += -1\n",
    "        elif actual_direction == \"up\":\n",
    "            if self.y==self.y_dim-1:\n",
    "                self.reward += self.crashed_reward\n",
    "            else:\n",
    "                self.y += 1\n",
    "        elif actual_direction == \"down\":\n",
    "            if self.y==0:\n",
    "                self.reward += self.crashed_reward\n",
    "            else:\n",
    "                self.y += -1\n",
    "        else:\n",
    "            raise RuntimeError(f\"unknown_direction: {actual_direction}\")\n",
    "\n",
    "        # Monsters\n",
    "        if (self.x,self.y) in self.monster_locs and flip(self.monster_appears_prob):\n",
    "            if self.damaged:\n",
    "                self.reward += self.monster_reward_when_damaged\n",
    "            else:\n",
    "                self.damaged = True\n",
    "        if (self.x,self.y) in self.repair_stations:\n",
    "            self.damaged = False\n",
    "\n",
    "        # Prizes\n",
    "        if (self.x,self.y) == self.prize:\n",
    "            self.reward += self.prize_reward\n",
    "            self.prize = None\n",
    "\n",
    "        # Statistics\n",
    "        self.number_steps += 1\n",
    "        self.accumulated_rewards += self.reward\n",
    "        if self.accumulated_rewards < self.min_accumulated_rewards:\n",
    "            self.min_accumulated_rewards = self.accumulated_rewards\n",
    "            self.min_step = self.number_steps\n",
    "        if self.accumulated_rewards>0 and self.reward>self.accumulated_rewards:\n",
    "            self.zero_crossing = self.number_steps\n",
    "        \n",
    "        self.display(2,\"\",self.number_steps,self.accumulated_rewards,\n",
    "                      self.accumulated_rewards/self.number_steps,sep=\"\\t\")\n",
    "\n",
    "        return self.reward, (self.x, self.y, self.damaged, self.prize)\n",
    "        \n",
    "    ### For GUI\n",
    "    def state2pos(self,state):\n",
    "        \"\"\"the (x,y) position for the state\n",
    "        \"\"\"\n",
    "        (x, y, damaged, prize) = state\n",
    "        return (x,y)\n",
    "        \n",
    "    def state2goal(self,state):\n",
    "        \"\"\"the (x,y) position for the goal\n",
    "        \"\"\"\n",
    "        (x, y, damaged, prize) = state\n",
    "        return prize\n",
    "        \n",
    "    def pos2state(self,pos):\n",
    "        \"\"\"the state corresponding to the (x,y) position.\n",
    "        The damages and prize are not shown in the GUI\n",
    "        \"\"\"\n",
    "        (x,y) = pos\n",
    "        return (x, y, self.damaged, self.prize)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024f5e21-7f8e-4b51-89a5-37ecc8237665",
   "metadata": {},
   "source": [
    "# Rolling Average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5f3401-bdc1-4199-ab42-040cc88e2c20",
   "metadata": {},
   "source": [
    " - Given a sequence of numerical values, $v_1,v_2,v_3,...$, predict the *mean* after the first $k$ values for each $k$.\n",
    " - The **rolling average**, $A_k$, is the mean of the first $k$ data points:\n",
    "   $$\n",
    "     A_k = \\frac{v_1 + ... + v_k}{k}\\,\n",
    "   $$\n",
    "\n",
    "![](figs_rl/fig08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103d156-904d-46ba-8389-ad6cca451521",
   "metadata": {},
   "source": [
    "# Temporal Differences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6623415a-7885-4069-a391-9969781922ee",
   "metadata": {},
   "source": [
    "- The difference, $v_k - A_{k-1}$, is called the **temporal difference (TD) error**\n",
    "- Specifies how different the new value, $v_k$, is from the old prediction, $A_{k-1}$\n",
    "- The old estimate, $A_{k-1}$, is updated $\\alpha_k$ times the TD error to get the new estimate $A_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35486f-4163-41a4-91cb-faf626baad72",
   "metadata": {},
   "source": [
    "- In reinforcement learning, the values are often estimates of the effects of actions\n",
    "    - more recent values are more accurate than earlier values because the agent is learning, and so they should be weighted more.\n",
    "- Set $\\alpha_k = (r+1)/(r+k)$, for $r \\gt 0$\n",
    "- If $r = 9$, $\\alpha_k = (10)/(9+k)$\n",
    "- For the first experience, $\\alpha_1 = 1$, so it ignores the prior $A_0$\n",
    "- After 11 experiences, $\\alpha_{11} = 0.5$, so it weights that experience as equal to all of its prior experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f5a69-c157-4a24-9cf1-ed4faa5f51ba",
   "metadata": {},
   "source": [
    "# Learning from Experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94e5fb-7595-49af-9d0e-033042e565d7",
   "metadata": {},
   "source": [
    " - Agent tries to learn the optimal policy from its history of interaction with the environment\n",
    "     - $⟨s_0,a_0,r_1,s_1,a_1,r_2,s_2,a_2,r_3,s_3,a_3,r_4,s_4⁢…⟩$\n",
    "- A sequence of experiences   ⟨s,a,r,s′⟩\n",
    "- Agent needs to learn from these experiences\n",
    "  - Aim of the agent is to maximize the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8f447-19a1-4503-8af7-f61eb1085625",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aea6d1-5eb3-4f05-9464-e9d913989b05",
   "metadata": {},
   "source": [
    " - $Q^*(s,a)$ : the expected value (cumulative discounted reward) of doing action $a$ in state $s$ and then following the optimal policy\n",
    " - **Q-learning** uses temporal differences to estimate the value of $Q^*(s,a)$\n",
    " - Agent maintains a table of $Q[S,A]$, where $S$ is the set of states and $A$ is the set of actions\n",
    " - $Q[S,A]$ represents its current estimate of $Q^*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1008d70d-f60c-4c5c-9872-f80f3973891d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc5a6bae-627e-4cd4-aadd-2907d5a5c047",
   "metadata": {},
   "source": [
    "![](figs_rl/fig01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4613eb3-7724-4ca5-972b-40afacaaffd1",
   "metadata": {},
   "source": [
    "## Q-learning controller"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a15db5-e74e-459c-bb73-7cb760c78a40",
   "metadata": {},
   "source": [
    " - array N[s,a]\n",
    "    - counts the number of times action $a$ was performed in state $s$\n",
    " - The function alpha_fun computed $\\alpha$ from the count\n",
    " - $alpha\\_fun(c) = 10/(9 + c)$ used often\n",
    " - If $\\alpha$ is fixed, the count array need not be maintained "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178dbec1-bc6d-4ff0-b5c1-2104faf7a5e9",
   "metadata": {},
   "source": [
    "![](figs_rl/fig03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b85468-0bbe-4e03-9da2-4d98e7fa4877",
   "metadata": {},
   "source": [
    "- The Q-learner learns an approximation of the optimal Q-function as long as the agent explores enough\n",
    "- and no bound on the number of times it tries an action in any state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793fdf1-17e0-48d8-bf2b-d4b10c0b3247",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "- Suppose Sam wanted to make an informed decision about whether to party or relax over the weekend.\n",
    "- Sam prefers to party, but is worried about getting sick. \n",
    "- Sam estimates the rewards to be the following irrespective of the resulting state\n",
    "\n",
    "| S        |       A | Reward |\n",
    "|:--------:|:--------:|:--------:|\n",
    "|  healthy |  relax   |  7    |\n",
    "|  healthy |  party   |  10    |\n",
    "|  sick    |  relax   |  0    |\n",
    "|  sick    |  party   |  2    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f5bb1-71c3-45fb-add5-537496a6fe80",
   "metadata": {},
   "source": [
    " - The agent doesn't know the model and learns from the $s, a, r, s'$ experiences\n",
    " - With a discount $\\gamma = 0.8$, $\\alpha = 0.3$, and $Q$ initially 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc60459-d282-4bf4-a685-d4a71291f0e0",
   "metadata": {},
   "source": [
    "![](figs_rl/fig02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77e75c-30aa-4cac-b2d5-dfe86b124092",
   "metadata": {},
   "source": [
    "- With $\\alpha$ fixed, the Q-values will approximate, but not converge to, the values obtained with value iteration\n",
    "- The smaller $\\alpha$ is, the closer it will converge to the actual Q-values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8cef6-d88e-4f09-934c-747c5003837a",
   "metadata": {},
   "source": [
    "# Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfa6dfe-39e1-41c5-aafa-7a73401e1b89",
   "metadata": {},
   "source": [
    "- Q-function estimate is not enough to determine what the agent should actually do\n",
    "- Two competing goals for an agent\n",
    "  - **Exploit** the knowledge that it has found to get higher rewards\n",
    "    - do one of the actions that maximizes Q[s,a] by doing one of the actions *a* in state *s*\n",
    "  - **Explore** to build a better estimate of the Q-function\n",
    "    - selecting an action at random at each time\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92bff6-aef3-4d2f-982c-9961813ef764",
   "metadata": {},
   "source": [
    "- Combine exploitation and exploration\n",
    "  - **$\\epsilon$-greedy exploration strategy**, where $0 \\le \\epsilon \\le 1$\n",
    "  - **softmax** action selection\n",
    "  - **upper confidence bound** method\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b83004a-618d-48c4-a703-532ca8138bfb",
   "metadata": {},
   "source": [
    "## Epsilon-Greedy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3ab4c-77fb-4416-99db-6f7c9b7c5381",
   "metadata": {},
   "source": [
    " - The agent selects\n",
    "    - a random action $\\epsilon$ of the time ($0 \\le \\epsilon \\le 1$)\n",
    "    - otherwise, an action that maximizes Q[s,a]\n",
    " - $\\epsilon$ may change over time\n",
    "    - in the early stages, an agent should act more randomly to encourage initial exploration\n",
    "    - as time progresses, the agent should act more greedily by reducing $\\epsilon$   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a778f98f-494b-4789-be08-0454da8e0061",
   "metadata": {},
   "source": [
    "## Softmax selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d07c1-3896-41cf-a2d4-5cb3fea7e41f",
   "metadata": {},
   "source": [
    "- Problem with $\\epsilon$-greedy strategy\n",
    "  - treats all the actions, apart from the best action, equivalently\n",
    "  - if some actions are seemingly more promising than others, put more effort in exploring those\n",
    "- Select action *a* with a probability depending on the value of Q[s,a]\n",
    "- Use **Gibbs** or **Boltzmann distribution**, where the probability of selecting action *a* in state *s* is proportional to $e^{Q[s,a]/\\tau}$\n",
    "- In state *s*, the agent selects action *a* with probability\n",
    "\n",
    "  $$\n",
    "    \\frac{e^{Q[s,a]/\\tau}}{\\Sigma_a e^{Q[s,a]/\\tau}}\\,\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04a798-e35a-404a-9574-88d08a6acb32",
   "metadata": {},
   "source": [
    "- $\\tau$ is the *temperature* that specifies how randomly values should be chosen\n",
    "- When $\\tau$ is high, the actions are chosen in almost equal amounts\n",
    "- As the temperature is reduced, higher-valued actions are likely to be chosen\n",
    "- In the limit as $\\tau \\rightarrow 0$, the best action is always chosen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa4a943-9f66-45cb-99e4-919f1044256c",
   "metadata": {},
   "source": [
    "## Upper Confidence Bound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a9637-35b4-4961-8ceb-b91af6a406be",
   "metadata": {},
   "source": [
    "- Problem with the above methods\n",
    "  - they do not distinguish the actions that the agent has tried many times for which there is a good estimate of the actual Q-value, from those actions that have not been tried much for whom the estimate is quite poor\n",
    "- Model a distribution of the uncertainty of the estimate of the expected values, not just the current expected value\n",
    "- The **upper confidence bound** is an upper estimate of the expected value\n",
    "    - is the sum of two terms, the Q estimate and a confidence bound that depends on the number of times the action has been chosen\n",
    "- N[s,a]\n",
    "  - the number of times action *a* has been selected for state *s*\n",
    "- N(s) = $\\Sigma_a$ N[s,a]\n",
    "  - the number of times state *s* has been visited       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed185cd-8975-442f-a315-7c4162ecba1e",
   "metadata": {},
   "source": [
    "$$\n",
    "U⁢C⁢B⁢1⁢(s,a)=Q⁢[s,a]+C * \\sqrt{\\frac{log ⁡N⁢(s)}{N⁢[s,a]}}\\,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def37df-e2d6-477a-965f-b0516049ef89",
   "metadata": {},
   "source": [
    "- $C$ is a constant that depends on the magnitude of the Q-values  (e.g., $\\sqrt{2}$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b318ca1f-8171-47ab-bbe8-3eeb82c94025",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eba254-acbf-4053-81be-66dddf24e937",
   "metadata": {},
   "source": [
    "- *State* is the state that action is chosen for\n",
    "- *Qs* is the {action : q_value} dictionary for the state\n",
    "- *Vs* is a {action : visits} dictionary for the current state\n",
    "    - where *visits* is the number of times that the action has been carried out in the current state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a74c6da1-bf1d-43e3-b4fb-bc3db210e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(state, Qs, Vs={}, epsilon=0.2):\n",
    "        \"\"\"select action given epsilon greedy\n",
    "        Qs is the {action:Q-value} dictionary for current state\n",
    "        Vs is ignored\n",
    "        epsilon is the probability of acting randomly\n",
    "        \"\"\"\n",
    "        if flip(epsilon):\n",
    "            return random.choice(list(Qs.keys())) # act randomly\n",
    "        else:\n",
    "            return argmaxd(Qs) # pick an action with max Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "055c34ec-7fde-4a89-bc93-f4d02e4aa782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ucb(state, Qs, Vs, c=1.4):\n",
    "        \"\"\"select action given upper-confidence bound\n",
    "        Qs is the  {action:Q-value} dictionary for current state\n",
    "        Vs is the {action:visits} dictionary for current state\n",
    "\n",
    "        0.01 is to prevent divide-by zero when Vs[a]==0\n",
    "        \"\"\"\n",
    "        Ns = sum(Vs.values())\n",
    "        ucb1 = {a:Qs[a]+c*math.sqrt(Ns/(0.01+Vs[a]))\n",
    "                    for a in Qs.keys()}\n",
    "        action = argmaxd(ucb1)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea176926-2298-4310-926b-e8ff4ba9420b",
   "metadata": {},
   "source": [
    "## Q-learner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aed66ac2-a36b-4f9c-856a-db17d4744f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learner(RL_agent):\n",
    "    \"\"\"A Q-learning agent has\n",
    "    belief-state consisting of\n",
    "        state is the previous state (initialized by RL_agent\n",
    "        q is a {(state,action):value} dict\n",
    "        visits is a {(state,action):n} dict.  n is how many times action was done in state\n",
    "        acc_rewards is the accumulated reward\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, role, actions, discount,\n",
    "                 exploration_strategy=epsilon_greedy, es_kwargs={},\n",
    "                 alpha_fun=lambda _:0.2,\n",
    "                 Qinit=0, method=\"Q_learner\"):\n",
    "        \"\"\"\n",
    "        role is the role of the agent (e.g., in a game)\n",
    "        actions is the set of actions the agent can do\n",
    "        discount is the discount factor\n",
    "        exploration_strategy is the exploration function, default \"epsilon_greedy\"\n",
    "        es_kwargs is extra arguments of exploration_strategy \n",
    "        alpha_fun is a function that computes alpha from the number of visits\n",
    "        Qinit is the initial q-value\n",
    "        method gives the method used to implement the role (for plotting)\n",
    "        \"\"\"\n",
    "        RL_agent.__init__(self, actions)\n",
    "        self.role = role\n",
    "        self.discount = discount\n",
    "        self.exploration_strategy = exploration_strategy\n",
    "        self.es_kwargs = es_kwargs\n",
    "        self.alpha_fun = alpha_fun\n",
    "        self.Qinit = Qinit\n",
    "        self.method = method\n",
    "        self.acc_rewards = 0\n",
    "        self.Q = {}\n",
    "        self.visits = {}\n",
    "\n",
    "    def initial_action(self, state):\n",
    "        \"\"\" Returns the initial action; selected at random\n",
    "        Initialize Data Structures\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.Q[state] = {act:self.Qinit for act in self.actions}\n",
    "        self.visits[state] = {act:0 for act in self.actions}\n",
    "        self.action = self.exploration_strategy(state, self.Q[state],\n",
    "                                     self.visits[state],**self.es_kwargs)\n",
    "        self.display(2, f\"Initial State: {state} Action {self.action}\")\n",
    "        self.display(2,\"s\\ta\\tr\\ts'\\tQ\")\n",
    "        return self.action\n",
    "        \n",
    "    def select_action(self, reward, next_state):\n",
    "        \"\"\"give reward and next state, select next action to be carried out\"\"\"\n",
    "        if next_state not in self.visits:  # next state not seen before\n",
    "            self.Q[next_state] = {act:self.Qinit for act in self.actions}\n",
    "            self.visits[next_state] = {act:0 for act in self.actions}\n",
    "        self.visits[self.state][self.action] +=1\n",
    "        alpha = self.alpha_fun(self.visits[self.state][self.action])\n",
    "        self.Q[self.state][self.action] += alpha*(\n",
    "                            reward\n",
    "                            + self.discount * max(self.Q[next_state].values())\n",
    "                            - self.Q[self.state][self.action])\n",
    "        self.display(2,self.state, self.action, reward, next_state, \n",
    "                     self.Q[self.state][self.action], sep='\\t')\n",
    "        self.action = self.exploration_strategy(next_state, self.Q[next_state],\n",
    "                                     self.visits[next_state],**self.es_kwargs)\n",
    "        self.state = next_state\n",
    "        self.display(3,f\"Agent {self.role} doing {self.action} in state {self.state}\")\n",
    "        return self.action\n",
    "\n",
    "    def q(self,s,a):\n",
    "        if s in self.Q and a in self.Q[s]:\n",
    "            return self.Q[s][a]\n",
    "        else:\n",
    "            return self.Qinit\n",
    "            \n",
    "    def v(self,s):\n",
    "        if s in self.Q:\n",
    "            return max(self.Q[s].values())\n",
    "        else:\n",
    "            return self.Qinit\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11b39b-d6b2-4ee0-b77d-fcaa5c8a4de8",
   "metadata": {},
   "source": [
    "## Example - Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0063ba1e-f11f-4af9-8302-4244490784b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.1780976396171 66.91778321587559 \n",
      "52.641191842471045 55.158532065879676 \n"
     ]
    }
   ],
   "source": [
    "env = Party_env()\n",
    "\n",
    "agent1 = Q_learner(env.name, env.actions, discount = 0.9, \n",
    "               alpha_fun = lambda k : 10/(9+k))\n",
    "sim = Simulate(agent1, env).start()\n",
    "sim.go(100000)\n",
    "\n",
    "states = ['healthy', 'sick']\n",
    "for s in states:\n",
    "    for a in env.actions:\n",
    "        print(agent1.q(s,a), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b6ed9a9-33d4-4da1-8695-c883fb1069cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: healthy Action party\n",
      "s\ta\tr\ts'\tQ\n",
      "step=1 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t3.0\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=2 reward=10, state=sick\n",
      "healthy\tparty\t10\tsick\t5.1\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=3 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t1.224\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=4 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t7.794\n",
      "Agent Party Decision doing relax in state healthy\n",
      "      action=relax\n",
      "step=5 reward=7, state=sick\n",
      "healthy\trelax\t7\tsick\t2.39376\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=6 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t2.72736\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=7 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t10.32636\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=8 reward=10, state=sick\n",
      "healthy\tparty\t10\tsick\t10.8830184\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=9 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t4.521076416\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=10 reward=10, state=sick\n",
      "healthy\tparty\t10\tsick\t11.70317121984\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n"
     ]
    }
   ],
   "source": [
    "random.seed(321)\n",
    "\n",
    "env = Party_env()\n",
    "env.max_display_level = 3\n",
    "\n",
    "agent1 = Q_learner(env.name, env.actions, discount = 0.8, \n",
    "               alpha_fun = lambda k : 0.3)\n",
    "agent1.max_display_level = 3\n",
    "\n",
    "sim = Simulate(agent1, env).start()\n",
    "sim.max_display_level = 3\n",
    "\n",
    "sim.go(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330a2f5-2576-40e3-996f-0c4d1001c914",
   "metadata": {},
   "source": [
    "# On-Policy Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5775512e-b998-4c40-b841-1a0dd02c516c",
   "metadata": {},
   "source": [
    "- Q-learning is an **off-policy** learner\n",
    "    - learns the value of an optimal policy independently of the agent's actions, as long as it explores enough\n",
    "    -  can learn optimal policy even it is acting randomly\n",
    "    -  However, it does not learn the value of the policy it is following, because it included exploration steps\n",
    "- Drawback of an off-policy learner\n",
    "    - When there are large negative rewards, ignoring what the agent actually does is dangerous\n",
    "- Alternative approach\n",
    "    - Learn the value of the policy the agent is actually carrying out, which includes exploration steps, so that the policy can be iteratively improved\n",
    "    - Take into account the costs associated with exploration\n",
    "-   **on-policy learner** learns the value of the policy being carried out by the agent, including the exploration steps    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684d2189-bd79-42a7-89a5-a0ea4f687d0d",
   "metadata": {},
   "source": [
    "## SARSA learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abb1a4-0b20-497f-af23-7ac743588b94",
   "metadata": {},
   "source": [
    " - Uses *state-action-reward-state-action* experiences to update Q-values\n",
    " - An **on-policy** reinforcement learning algorithm that estimates the value of the policy being followed\n",
    " - An experience is of the form ⟨s,a,r,s′,a′⟩\n",
    "     - the agent in state *s* did action *a*, received reward *r*, and ended up in state $s'$, from which it decided to do action $a'$\n",
    "     - this experience is used to update Q(s,a)\n",
    " - The new value this experience provides is\n",
    "     - r + $\\gamma$ ⁢Q⁢(s′,a′)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab3ae5-e40e-4ac1-bb62-9952934aceb0",
   "metadata": {},
   "source": [
    "![](figs_rl/fig04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38928860-51fe-4c83-9eb2-47eeb487d37f",
   "metadata": {},
   "source": [
    "## SARSA-learner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a59590b-e874-4bf5-b896-d8e38cdb8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Q_learner):\n",
    "    def __init__(self,*args, **nargs):\n",
    "        Q_learner.__init__(self,*args, **nargs)\n",
    "        self.method = \"SARSA\"\n",
    "        \n",
    "    def select_action(self, reward, next_state):\n",
    "        \"\"\"give reward and next state, select next action to be carried out\"\"\"\n",
    "        if next_state not in self.visits:  # next state not seen before\n",
    "            self.Q[next_state] = {act:self.Qinit for act in self.actions}\n",
    "            self.visits[next_state] = {act:0 for act in self.actions}\n",
    "        self.visits[self.state][self.action] +=1\n",
    "        alpha = self.alpha_fun(self.visits[self.state][self.action])\n",
    "        next_action = self.exploration_strategy(next_state, self.Q[next_state],\n",
    "                                     self.visits[next_state],**self.es_kwargs)\n",
    "        self.Q[self.state][self.action] += alpha*(\n",
    "                            reward\n",
    "                            + self.discount * self.Q[next_state][next_action]\n",
    "                            - self.Q[self.state][self.action])\n",
    "        self.display(2,self.state, self.action, reward, next_state, \n",
    "                     self.Q[self.state][self.action], sep='\\t')\n",
    "        self.state = next_state\n",
    "        self.action = next_action\n",
    "        self.display(3,f\"Agent {self.role} doing {self.action} in state {self.state}\")\n",
    "        return self.action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b710f0-9819-4312-8c6f-533acc37552e",
   "metadata": {},
   "source": [
    "## Example - Party"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49c854d3-f038-41c5-b6c3-ee0032550205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.21525047283787 65.99113308324256 \n",
      "51.707859767761775 54.08535958884556 \n"
     ]
    }
   ],
   "source": [
    "env = Party_env()\n",
    "\n",
    "agent2 = SARSA(env.name, env.actions, discount = 0.9, \n",
    "               alpha_fun = lambda k : 10/(9+k))\n",
    "sim = Simulate(agent2, env).start()\n",
    "sim.go(100000)\n",
    "\n",
    "states = ['healthy', 'sick']\n",
    "for s in states:\n",
    "    for a in env.actions:\n",
    "        print(agent2.q(s,a), end=' ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "040a947f-3618-4395-8722-b46421deb27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State: healthy Action party\n",
      "s\ta\tr\ts'\tQ\n",
      "step=1 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t10.0\n",
      "Agent Party Decision doing relax in state healthy\n",
      "      action=relax\n",
      "step=2 reward=7, state=sick\n",
      "healthy\trelax\t7\tsick\t7.0\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=3 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t9.0\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=4 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t15.727272727272727\n",
      "Agent Party Decision doing relax in state healthy\n",
      "      action=relax\n",
      "step=5 reward=7, state=sick\n",
      "healthy\trelax\t7\tsick\t14.363636363636363\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=6 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t13.68595041322314\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=7 reward=10, state=healthy\n",
      "healthy\tparty\t10\thealthy\t22.75\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=8 reward=10, state=sick\n",
      "healthy\tparty\t10\tsick\t22.41719643992371\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n",
      "step=9 reward=0, state=healthy\n",
      "sick\trelax\t0\thealthy\t19.09388906547997\n",
      "Agent Party Decision doing party in state healthy\n",
      "      action=party\n",
      "step=10 reward=10, state=sick\n",
      "healthy\tparty\t10\tsick\t25.822413382072472\n",
      "Agent Party Decision doing relax in state sick\n",
      "      action=relax\n"
     ]
    }
   ],
   "source": [
    "random.seed(321)\n",
    "\n",
    "env = Party_env()\n",
    "env.max_display_level = 3\n",
    "\n",
    "agent2 = SARSA(env.name, env.actions, discount = 0.9, \n",
    "               alpha_fun = lambda k : 10/(9+k))\n",
    "agent2.max_display_level = 3\n",
    "\n",
    "sim = Simulate(agent2, env).start()\n",
    "sim.max_display_level = 3\n",
    "\n",
    "sim.go(10);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8714017b-a4a9-42cc-aba2-565c3a8f03ee",
   "metadata": {},
   "source": [
    "## Example - Monster Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a023f861-6342-4209-9304-a828dc3a9596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(321)\n",
    "\n",
    "mon_env = Monster_game_env()\n",
    "\n",
    "mag1 = Q_learner(mon_env.name, mon_env.actions, 0.9,\n",
    "                     method=\"alpha=0.2\")\n",
    "#Simulate(mag1,mon_env).start().go(100000).plot()\n",
    "\n",
    "\n",
    "mag_ucb = Q_learner(mon_env.name, mon_env.actions, 0.9,\n",
    "                        exploration_strategy = ucb, es_kwargs={'c':0.1}, method=\"UCB(0.1),alpha=0.2\")\n",
    "#Simulate(mag_ucb,mon_env).start().go(100000).plot()\n",
    "\n",
    "\n",
    "mag2 = Q_learner(mon_env.name, mon_env.actions, 0.9,\n",
    "                     alpha_fun=lambda k:1/k,method=\"alpha=1/k\")\n",
    "#Simulate(mag2,mon_env).start().go(100000).plot()\n",
    "\n",
    "\n",
    "mag3 = Q_learner(mon_env.name, mon_env.actions, 0.9,\n",
    "                     alpha_fun=lambda k:10/(9+k), method=\"Q; alpha=10/(9+k)\")\n",
    "Simulate(mag3,mon_env).start().go(100000).plot()\n",
    "\n",
    "mag3_1 = SARSA(mon_env.name, mon_env.actions, 0.9,\n",
    "                     alpha_fun=lambda k:10/(9+k), method=\"S; alpha=10/(9+k)\")\n",
    "Simulate(mag3_1,mon_env).start().go(100000).plot()\n",
    "\n",
    "\n",
    "mag4 = Q_learner(mon_env.name, mon_env.actions, 0.9,\n",
    "                 alpha_fun=lambda k:10/(9+k),\n",
    "                 exploration_strategy = ucb, es_kwargs={'c':0.1},\n",
    "                 method=\"ucb & alpha=10/(9+k)\")\n",
    "#Simulate(mag4,mon_env).start().go(100000).plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aefdc48-a72e-48c5-bb61-715a54bec0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09a37af-a412-4916-a463-f1f05681c2c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5bd583a-1a82-49d6-82a8-b8d10cacbc06",
   "metadata": {},
   "source": [
    "# RL with Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e21598-a6de-4c13-a405-42c96e36c479",
   "metadata": {},
   "source": [
    "- Alternative to reasoning explicitly in terms of states\n",
    "- Reason in terms of features, which can either be provided explicitly or learned\n",
    "- Generic reinforcement on-policy learner that incorporates a supervised learner\n",
    "- The learner carries out the following operations\n",
    "    - *add(x,y)* which adds a new example to the dataset, with input $x$ and target value $y$\n",
    "    - *predict(x)* which gives a point prediction for the target for an example with input $x$\n",
    "- In the following, the input $x$ for the learner is a *state-action* pair, and the target for pair $(s,a)$ is an estimate of $Q^*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53404797-4092-476f-95be-7750154bfae5",
   "metadata": {},
   "source": [
    "![](figs_rl/fig06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9838c1e2-e3c9-495c-8f80-d98e2d359bde",
   "metadata": {},
   "source": [
    "# SARSA with Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd3fd37-221a-4af7-8eb5-aad100e0da85",
   "metadata": {},
   "source": [
    "- A linear function of features for the state and the action\n",
    "- Require more information about the domain in terms of features\n",
    "- Based on incremental gradient descent that updates the parameters after every example\n",
    "- Suppose $F_1, ..., F_n$ are numerical features of the state and the action\n",
    "    - $F_i(s,a)$ provides the value for the $i^{th}$ feature for state *s* and action *a*\n",
    "- Used in representing the linear Q-function\n",
    "\n",
    "$$\n",
    "  Q_{\\bar{w}}(s,a) = w_0 + w_1 * F_1(s,a) + ... + w_n * F_n(s,a)\n",
    "$$\n",
    "\n",
    "for some tuple of weights $\\bar{w} = ⟨w_0,w_1,…,w_n⟩$ that have to be learned "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f7dd00-5640-4bbc-af66-0aea288984ed",
   "metadata": {},
   "source": [
    "- An experience in SARSA of the form ⟨s,a,r,s',a'⟩ provides the new estimate $r + \\gamma * Q_{\\bar{w}}(s',a')$ to update $Q_{\\bar{w}}(s,a)$\n",
    "- used as a data point for **linear regression**\n",
    "- Let $\\delta = Q_{\\bar{w}}(s,a) - (r + \\gamma * Q_{\\bar{w}}(s',a')$\n",
    "- The weight $w_i$ is updated by\n",
    "  $$\n",
    "    w_i = w_i - \\eta * \\delta * F_i(s,a)\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b158e5a-3546-4f59-ad6a-036390b23daa",
   "metadata": {},
   "source": [
    "![](figs_rl/fig07.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c3ed5-f1ac-4ce5-b04f-94af860f6e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
