install.packages(c("rmarkdown", "knitr"))
#install.packages("asbio")
# https://cran.r-project.org/web/packages/asbio/index.html
library(asbio)
#Compute z*
ConfLevel <- 0.99 #Define confidence level
LeftTail <- ConfLevel+(1-ConfLevel)/2
zStar <- qnorm(LeftTail)
rnorm(100,mean = 114, sd = 514)
rnorm(100,mean = 114, sd = i)
rnorm(100,mean = 114, sd = -1)
rnorm(100,mean = 114, sd = 514)
quantile(rnorm(100,mean = 114, sd = 514))
mean(rnorm(100,114,514))
mean(rnorm(100,1,10000))
mean(rnorm(100,1,10000))
mean(rnorm(100,1,10000))
mean(rnorm(100,1,10000))
sd(rnorm(100,1,100))
sd(rnorm(100,1,10000))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
sd(rnorm(100,114,514))
#Compute z*
ConfLevel <- 0.99 #Define confidence level
LeftTail <- ConfLevel+(1-ConfLevel)/2
zStar <- qnorm(LeftTail)
dim()
a <- rnorm(rnorm(100,114,514) )
b <- sample(a, 49)
mean(b)
mean(a)
a <- rnorm(100,114,514)
b <- sample(a, 49)
mean(b)
mean(a)
b <- sample(a, 64)
mean(a)
mean(b)
3.6/6
3.5/6
2.576*3.5/6
92.83735-1.96*(sd(b)/8)
sd(b)
sd(a)
92.83735-1.96*(sd(a)/8)
1.96*sd(a)/8
a<-rnorm(100,10,10)
b<-sample(a,36)
sd(a)
sd(b)
mean(b)+1.96*10/8
mean(b)-1.96*10/8
mea(a)
mean(a)
mean(b)-1.645*10/8
mean(b)+1.645*10/8
qnorm(.1,lower.tail = F)
mean(b)-1.28*10/8
mean(b)+1.28*10/8
qnorm(.25,lower.tail = F)
(15/1*1.96)^2
ceiling((15/1*1.96)^2)
?one.sample.z
sd(a)
SD<-sd(a)
b <- sample(a,36)
mean(b)
bmean <- mean(b)
bmean <- 11
se = SD/6
critZ <- (mean(b)-bmean)/se
critZ <- (mean(b)-20)/se
one.sample.z(null.mu = 0, xbar = -2.98, sigma = 6, n=30,alternative = "one.sided")
one.sample.z(null.mu = 0, xbar = -2.98, sigma = 6, n=30,alternative = "less")
one.sample.z(null.mu = 155, xbar = 117, sigma = 30, n=100,alternative = "two.sided")
one.sample.z(null.mu = 115, xbar = 117, sigma = 30, n=100,alternative = "two.sided")
##Critical value approach
1.5967 > qnorm(0.975)
##P value approach
##replicate p value
pnorm(1.597, lower.tail = F)*2
##compare p value with alpha level
0.11 < 0.05
help("fit")
??fit
library(python)
install.packages("reticulate")
one.sample.z(null.mu = 42, xbar = 38.37524, sigma = 23.74226, n=36,alternative = "two.sided")
one.sample.z(null.mu = 42, xbar = 38.37524, sigma = 23.74226, n=36,alternative = "two.si")
one.sample.z(null.mu = 42, xbar = 38.37524, sigma = 23.74226, n=36,alternative = "less")
sigma <- 169
(xbar <- mean(sample))
(z <- qnorm(0.975))
(cl <- c(xbar-z*sigma/sqrt(40), xbar+z*sigma/sqrt(40)))
one.sample.z(null.mu=919, xbar=xbar, sigma=169,
n=40, alternative="two.sided", conf=0.95)
z <- qnorm(0.975)
sigma <- 4
m <- 0.5
n <- (z*sigma/m)^2
n
one.sample.z(null.mu=10, xbar=10.8, sigma=1,
n=30, alternative="greater", conf=0.95)
one.sample.z(null.mu=10, xbar=10.8, sigma=1,
n=30, alternative="greater")
zval <- (16.4-15)/(6.2/sqrt(50))
(zval <- (16.4-15)/(6.2/sqrt(50)))
##Critical value approach
1.5967 > qnorm(0.975)
kid_cal <- read.csv("kid_cal.csv")
summary(kid_cal)
Kid_mean <- kid_cal$trt
Kid_meal <- kid_cal$trt = T
kid_cal <- read.csv("kid_cal.csv")
Kid_meal <- kid_cal$trt = T
Kid_meal <- kid_cal[kid_cal$trt = T]
Kid_meal <- kid_cal[kid_cal$trt == T]
Kid_meal <- kid_cal$Calories[kid_cal$trt == T]
Kid_no_meal <- kid_cal$Calories[kid_cal$trt == F]
hist(Kid_meal)
par(mfrow = c(2,2))
hist(Kid_meal)
hist(Kid_no_meal)
knitr::opts_chunk$set(echo = TRUE)
hist(Kid_no_meal, main = "Calorie Distribution for Non-Participants",
xlab = "Calorie", freq = F)
hist(Kid_no_meal, main = "Calorie Distribution for Non-Participants",
xlab = "Calorie", ylab = "Child Count")
knitr::opts_chunk$set(echo = TRUE)
kid_cal <- read.csv("kid_cal.csv")
Kid_meal <- kid_cal$Calories[kid_cal$trt == T]
Kid_no_meal <- kid_cal$Calories[kid_cal$trt == F]
mealFrame <- data.frame(
Mean = mean(Kid_meal),
Median = median(Kid_meal),
SD = sd(Kid_meal),
First_Quantile = quantile(Kid_meal,.25)[[1]],
Third_Quantile = quantile(Kid_meal,.75)[[1]],
Min = min(Kid_meal),
Max = max(Kid_meal)
)
daysTable <- kable(mealFrame,"simple")
knitr::opts_chunk$set(echo = TRUE)
library(kableExtra)
kid_cal <- read.csv("kid_cal.csv")
Kid_meal <- kid_cal$Calories[kid_cal$trt == T]
Kid_no_meal <- kid_cal$Calories[kid_cal$trt == F]
mealFrame <- data.frame(
Mean = mean(Kid_meal),
Median = median(Kid_meal),
SD = sd(Kid_meal),
First_Quantile = quantile(Kid_meal,.25)[[1]],
Third_Quantile = quantile(Kid_meal,.75)[[1]],
Min = min(Kid_meal),
Max = max(Kid_meal)
)
daysTable <- kable(mealFrame,"simple")
par(mfrow = c(2,2))
hist(Kid_meal, main = "Calorie Distribution for Participants",
xlab = "Calorie", ylab = "Child Count")
hist(Kid_no_meal, main = "Calorie Distribution for Non-Participants",
xlab = "Calorie", ylab = "Child Count")
(mealTable <- kable(mealFrame,"simple"))
(mealTable <- kable(mealFrame,"simple", label = "Summary of Calorie for Participants of Meal Preparation"))
help("qt")
qt(0.9, df=9)
pt(1.383, df=9)
# Shark length example
# A scientist wishes to test the claim that great white sharks average 20 feet in length.
# To test this, he measures 10 great white sharks.
shark_len <- c(18.1, 23.4, 23.9, 24.1, 22.5, 19, 25.4, 23.1, 16.5, 26.7)
xbar <- mean(shark_len)
s <- sd(shark_len)
n <- length(shark_len)
# calculate the t statistic
t <- (xbar-20)/(s/sqrt(n))
critical <- qt(0.9, df=n-1)
t
critical
# calculate the t statistic
t <- (xbar-20)/(s/sqrt(n))
# calculate the p-value , we are doing a one-sided greater t-test
(p <- 1 - pt(t, df=n-1))
xbar
s
1.383*3.314/(sqrt(10))
22.27+1.383*3.314/(sqrt(10))
22.27-1.383*3.314/(sqrt(10))
lb <- xbar - critical*s/sqrt(n)
lb
3.2-1.74*4.5/sqrt(18)
3.2+1.74*4.5/sqrt(18)
pt(1.74,17)
pt(0,17)
sqrt(18)*3.2/4.5
pt(3.017,17)
##CV approach
qt(0.05, 17, lower.tail = F)
822
no
763
# First read the data as a dataframe into your R memory
decay = read.csv("decay.csv")
# First read the data as a dataframe into your R memory
decay = read.csv("./decay.csv")
setwd("C:/BU/CSSE/CS555/L4/")
# First read the data as a dataframe into your R memory
decay = read.csv("./decay.csv")
# Print out a summary of the data for the 2 weeks sample data
summary(decay$strength[decay$weeks==2])
# Print out a summary of the data for the 16 weeks sample data
summary(decay$strength[decay$weeks==16])
a <- c(843,798,861,896,830,831,880,865,)
a <- c(843,798,861,896,830,831,880,865,884,882)
a <- c(843,798,861,896,830,831,880,865,884,882,783)
a <- c(843,798,861,896,830,831,880,865,882,783)
amean<-mean(a)
asd<-sd(a)
t.test(a)
t.test(a, mu = 850, alternative = "greater")
a <- c(843,798,861,896,830,831,880,865,884,783)
t.test(a, mu = 850, alternative = "greater")
##CV approach
qt(0.05, 17, lower.tail = F)
##CV approach
qt(0.05, 17, lower.tail = T)
# First read the data as a dataframe into your R memory
decay = read.csv("./decay.csv")
# Print out a summary of the data for the 2 weeks sample data
summary(decay$strength[decay$weeks==2])
# Print out a summary of the data for the 16 weeks sample data
summary(decay$strength[decay$weeks==16])
# Or you can do
aggregate(decay$strength, by =list(as.factor(decay$weeks)), FUN=summary)
aggregate(decay$strength, by =list(as.factor(decay$weeks)), FUN=sd)
##CV approach
n <- nrow(decay)/2
# Compute the test t statistic and the associated p-value
# Fail to reject H0 since p-value is greater than ??
t.test(decay$strength[decay$weeks==2], decay$strength[decay$weeks==16],
alternative="greater", conf.level=0.9)
#install.packages("gplots")
library(gplots)
#install.packages("gglots")
library(gglots)
#install.packages("ggplots")
library(ggplots)
library
library(gplots)
#install.packages("gplots")
library(gplots)
attach(decay)
m <- aggregate(strength, by=list(weeks), mean)
s <- aggregate(strength, by=list(weeks), sd)
m
s
## side by side boxplots
boxplot(strength~weeks)
## tapply function
(means <- tapply(strength, weeks, mean))
##How to use tapply to apply a user defined function "a"
##to data "strength" by "weeks"
##step1
##pick lower bound of confidence interval as
##an example
t.test(strength)$conf.int[1]
##How to use tapply to apply a user defined function "a"
##to data "strength" by "weeks"
##step1
##pick lower bound of confidence interval as
##an example
(t.test(strength)$conf.int[1])
t.test(strength)
##step2
##Define a user defined function "a"
##apply to strength by weeks
##We get the lowerbound for 2 week and 16 week respectively.
a <- function(v)
{
t.test(v)$conf.int[1]
}
tapply(strength, weeks, a)
# step3
##This is the combined version of step2
##Everything in one line.
##Also we added upperbound as well
(lower <- tapply(strength, weeks, function(v) t.test(v)$conf.int[1]))
(upper <- tapply(strength, weeks, function(v) t.test(v)$conf.int[2]))
##barplot2: An enhancement of the standard barplot() function.
##regular barplot
barplot(means, plot.ci=TRUE, ci.l=lower, ci.u=upper,
names.arg=c("2 weeks", "16 weeks"))
##see what is the difference using barplot2.
barplot2(means, plot.ci=TRUE, ci.l=lower, ci.u=upper,
names.arg=c("2 weeks", "16 weeks"))
abline(h=0)
help("tapply")
type(t.test(strength))
names(t.test(strength))
typeof(t.test(strength))
#Exercise 1
#Are the average test scores of male and female students significally different?
score <- read.csv("students_test_score.csv")
attach(score)
aggregate(Score, by=list(Gender), mean)
aggregate(Score, by=list(Gender), sd)
boxplot(Score~Gender)
t.test(Score[Gender=="F"], Score[Gender=="M"], alternative="two.sided", conf.level=0.95)
t.test(Score[Gender=="F"], Score[Gender=="M"], alternative="greater", conf.level=0.95)
means <- tapply(Score, Gender, mean)
lower <- tapply(Score, Gender, function(v) t.test(v)$conf.int[1])
upper <- tapply(Score, Gender, function(v) t.test(v)$conf.int[2])
barplot2(means, plot.ci=TRUE, ci.l=lower, ci.u=upper, names.arg=c("Female", "Male"))
abline(h=0)
x <- c(3, 2, 2, 5, 0)
xbar <- mean(x)
xbar
s <- sd(x)
s
se <- s/sqrt(5)
se
ci <- c(xbar-qt(0.975, 4)*se, xbar+qt(0.975, 4)*se)
ci
?barplot2
