{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca977c99-1a2c-496c-866a-0d6a63f7cdf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#for imageCount in range(0,101):\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m#    currpath = image_path+str(imageCount)+'.JPEG'\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iamgeName \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(image_path):\n\u001b[1;32m---> 27\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m preprocess(image)\n\u001b[0;32m     29\u001b[0m     image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add a batch dimension\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import glob\n",
    "# Load the pretrained InceptionV4 model\n",
    "# Note: InceptionV4 is not directly available in torchvision, so we use InceptionV3 as a close substitute for demonstration\n",
    "model = models.inception_v3(pretrained=True)\n",
    "model.eval()  # Set the model to inference mode\n",
    "\n",
    "# Define the image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(299),  # Resize the image to 299x299\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load and preprocess an image from Tiny ImageNet\n",
    "#image_path = 'data\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_'  \n",
    "image_path = 'data\\\\tiny-imagenet-200\\\\val\\\\images\\\\*'  \n",
    "image_list = []\n",
    "tag_list  = []\n",
    "#for imageCount in range(0,101):\n",
    "#    currpath = image_path+str(imageCount)+'.JPEG'\n",
    "for iamgeName in glob.glob(image_path):\n",
    "    image = Image.open(currpath).convert('RGB')\n",
    "    image = preprocess(image)\n",
    "    image = image.unsqueeze(0)  # Add a batch dimension\n",
    "    image_list.append(image)\n",
    "f = open('data\\\\tiny-imagenet-200\\\\val\\\\val_annotations.txt', 'r')\n",
    "\n",
    "for line in f:\n",
    "    x = f.readline()\n",
    "    y = x.split()\n",
    "    tag_list.append(y[1])\n",
    "print(tag_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "708b6145-6c0e-4425-b6db-35666279a62d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data\\\\tiny-imagenet-200\\\\val\\\\images\\\\val_10.JPEG'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imageName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3824c315-f348-4278-841c-ee778388e1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 680 with probability 0.3239869177341461\n",
      "nipple 0.3239869177341461\n"
     ]
    }
   ],
   "source": [
    "url, filename = (\"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\", \"imagenet_classes.txt\")\n",
    "urllib.request.urlretrieve(url, filename) \n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image)\n",
    "\n",
    "# The output has unnormalized scores. To get probabilities, apply softmax.\n",
    "probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "# Display the top 5 categories per the original ImageNet labels\n",
    "# Note: You would need to map these to Tiny ImageNet classes manually as discussed\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 1)\n",
    "for i in range(top5_prob.size(0)):\n",
    "    print(f\"{i+1}: {top5_catid[i].item()} with probability {top5_prob[i].item()}\")\n",
    "    print(categories[top5_catid[i]], top5_prob[i].item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
